import copy
import glob
import logging
import logging.config
import os
from itertools import chain
from pathlib import Path

import click
from natsort import natsorted

from job_creator.architectures import architecture_map
from job_creator.ci_objects import Job, Trigger, Workflow
from job_creator.containers import (Spacktainerizer,
                                    generate_base_container_workflow,
                                    generate_spacktainers_workflow)
from job_creator.job_templates import (clean_cache_yaml,
                                       generate_containers_workflow_yaml)
from job_creator.logging_config import LOGGING_CONFIG
from job_creator.packages import generate_packages_workflow
from job_creator.utils import (get_arch_or_multiarch_job, get_architectures,
                               load_yaml, write_yaml)

logging.config.dictConfig(LOGGING_CONFIG)
logger = logging.getLogger("job_creator")


def debug_output(debug):
    if debug:
        stream_handler = next(h for h in logger.handlers if h.name == "sh")
        stream_handler.setLevel(logging.DEBUG)


@click.group()
def jc():
    pass


@jc.command
@click.option(
    "--architecture",
    "-a",
    help="Architecture to generate spacktainer pipeline for",
)
@click.option(
    "--out-dir",
    "-o",
    help="Which directory to write the spacktainer build pipeline to",
)
@click.option("--s3cmd-version", "-s", default="2.3.0", help="s3cmd version")
def generate_spacktainer_workflow(architecture, out_dir, s3cmd_version):
    """
    Generate the workflow that will build the actual spack-package-based containers
    for the given container definition
    """
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    workflow = generate_spacktainers_workflow(architecture, out_dir, s3cmd_version)
    write_yaml(workflow.to_dict(), f"{out_dir}/spacktainer_pipeline.yaml")


@jc.command
@click.option(
    "--pipeline-dir",
    "-d",
    help="Directory containing YAML pipeline files generated by spack",
)
@click.option(
    "--out-dir",
    "-o",
    help="Output dir in which to dump split pipelines. Will be created if necessary.",
)
@click.option(
    "--debug/--no-debug", default=False, help="Show debug logging on the console"
)
def process_spack_pipeline(pipeline_dir, out_dir, debug):
    """
    Given a directory with spack-generated pipeline files, this will:
      * merge all of them into one giant pipeline file, keeping the earliest version of any duplicates
      * split it along the generated stages: each stage will become its own workflow
      * in each "stage", do the necessary spack mirror manipulation, variable setting, ...
      * add a job before all the stages run that will collect artifacts needed, so that
        the stages can grab them from within the same workflow
      * configure "stage" dependencies
    """
    debug_output(debug)
    logger.info("Processing spack pipeline")
    packages_count = 0
    noop_workflow = Workflow()
    workflow = Workflow()
    architecture = out_dir.split(".")[1]
    pipeline_dir = Path(pipeline_dir)
    out_dir = Path(out_dir)
    for generated_pipeline_file in chain(
        pipeline_dir.glob("*/*.yml"), pipeline_dir.glob("*/*.yaml")
    ):
        logger.info(
            f"Processing spack workflow for file {generated_pipeline_file} with output dir {out_dir}"
        )
        spack_generated_pipeline = load_yaml(generated_pipeline_file)

        noop_name = "no-specs-to-rebuild"
        if noop_name in spack_generated_pipeline:
            noop = spack_generated_pipeline[noop_name]
            if noop_name not in noop_workflow:
                noop_workflow.add_job(
                    Job(
                        name=noop_name,
                        architecture=architecture,
                        append_arch=False,
                        **spack_generated_pipeline[noop_name],
                    )
                )
                noop_workflow.rules = spack_generated_pipeline.get("workflow", {}).get(
                    "rules"
                )
            continue

        artifacts_root = Path(
            spack_generated_pipeline["variables"]["SPACK_ARTIFACTS_ROOT"]
        )
        container_name = artifacts_root.name
        for name, item in spack_generated_pipeline.items():
            if name == "variables":
                if "variables" in architecture_map[architecture]:
                    logger.debug(
                        f"Adding {architecture} variables to variables section"
                    )
                    item.update(architecture_map[architecture]["variables"])
                logger.debug("Setting workflow-level variables")
                workflow.variables.update(item)
                continue
            elif name == "workflow":
                logger.debug("Adding workflow rules")
                workflow.rules = item["rules"]
                continue
            elif name == "stages":
                logger.debug("Ignoring stages, we'll roll our own")
                continue
            logger.debug(f"Found an actual job: {name}")
            if name != "rebuild-index":
                packages_count += 1
            job = Job(name=name, architecture=architecture, append_arch=False, **item)
            logger.debug(f"Job came from the {container_name} population job")
            job.needs = [
                {
                    "pipeline": os.environ["CI_PIPELINE_ID"],
                    "job": f"generate build cache population job for {container_name} for {architecture}",
                    "artifacts": True,
                }
            ]
            job.add_spack_mirror()
            job.set_aws_variables()
            job.timeout = "4h"
            job.variables.update(spack_generated_pipeline["variables"])

            job.image["pull_policy"] = "always"
            workflow.add_job(job, special_spack_treatment=True)

    out_dir.mkdir(parents=True, exist_ok=True)
    if workflow.jobs:
        logger.debug("Sorting workflow stages and jobs")
        workflow.stages = natsorted(workflow.stages)
        workflow.jobs = natsorted(workflow.jobs, key=lambda x: f"{x.stage}-{x.name}")
        logger.debug("Writing to merged_spack_pipeline.yaml for debugging purposes")
        write_yaml(workflow.to_dict(), "merged_spack_pipeline.yaml")
    else:
        write_yaml(noop_workflow.to_dict(), "spack_pipeline.yaml")
        return

    logger.debug("Splitting into stage pipelines and generating trigger workflow")
    trigger_workflow = Workflow(rules=workflow.rules)
    collect_job = Job(
        "collect artifacts",
        architecture,
        needs=[
            {
                "pipeline": os.environ.get("CI_PIPELINE_ID"),
                "job": f"process spack pipeline for {architecture}",
                "artifacts": True,
            }
        ],
        script=[
            "cat spack_pipeline.yaml",
            f"find artifacts.{architecture}",
        ],
        stage="collect artifacts",
        artifacts={"when": "always", "paths": ["*.yaml", "artifacts.*"]},
        rules=[{"when": "always"}],
    )
    trigger_workflow.add_job(collect_job)

    previous_stage = None

    for stage in workflow.stages:
        logger.debug(f"Stage: {stage}")
        pipeline_file = out_dir / f"pipeline-{stage}.yaml"
        stage_workflow = Workflow(rules=workflow.rules, variables=workflow.variables)
        stage_workflow.jobs = [job for job in workflow.jobs if job.stage == stage]

        if not stage_workflow.jobs:
            logger.debug(f"No jobs for {stage} - skipping")
            continue

        for job in stage_workflow.jobs:
            job.stage = None

        write_yaml(stage_workflow.to_dict(), pipeline_file)

        needs = [
            {
                "job": collect_job.name,
                "artifacts": True,
            },
        ]

        if previous_stage:
            needs.append({"job": previous_stage})

        stage_trigger = Trigger(
            name=stage,
            trigger={
                "include": [
                    {
                        "artifact": str(pipeline_file),
                        "job": collect_job.name,
                    }
                ],
                "strategy": "depend",
            },
            needs=needs,
            stage=stage,
            rules=[{"when": "always"}],
        )

        trigger_workflow.add_trigger(stage_trigger)

        previous_stage = stage

    write_yaml(trigger_workflow.to_dict(), "spack_pipeline.yaml")
    logger.info(f"{packages_count} packages will be rebuilt.")


def generate_containers_workflow(existing_workflow, architectures, s3cmd_version):
    """
    Generate the jobs to build the spacktainer containers
    """
    builder = Spacktainerizer(name="builder", build_path="builder")

    workflow = Workflow()
    for architecture in architectures:
        arch_job = Job(
            "generate spacktainer jobs",
            architecture=architecture,
            force_needs=True,
            **copy.deepcopy(generate_containers_workflow_yaml),
        )
        arch_job.image = {
            "name": f"{builder.registry_image}:{builder.registry_image_tag}",
            "pull_policy": "always",
        }
        arch_job.needs.extend(
            [j.name for j in get_arch_or_multiarch_job(existing_workflow, architecture)]
        )
        arch_job.variables["ARCHITECTURE"] = architecture
        arch_job.variables["OUTPUT_DIR"] = f"artifacts.{architecture}"
        arch_job.variables["S3CMD_VERSION"] = s3cmd_version

        workflow.add_job(arch_job)
    return workflow


def generate_clean_cache_workflow(architectures, cache_population_job_names):
    """
    Generate the jobs to clean the build cache
    """
    workflow = Workflow()
    stage = "clean build cache"
    workflow.stages = [stage]
    for architecture in architectures:
        arch_job = Job(
            "clean build cache",
            architecture=architecture,
            stage=stage,
            **copy.deepcopy(clean_cache_yaml),
        )

        bucket_info = architecture_map[architecture]["cache_bucket"]
        for job_name in cache_population_job_names[architecture]:
            arch_job.needs.append(
                {
                    "job": job_name,
                    "artifacts": True,
                }
            )
        env_args = []
        arch_container_definitions = Path(f"container_definitions/{architecture}/")
        for container_definition in arch_container_definitions.glob("*yaml"):
            env_args.append(
                f"-e jobs_scratch_dir.{architecture}/{container_definition.stem}/concrete_environment/spack.lock"
            )
        arch_job.variables = {
            "SPACK_ENV_ARGS": " ".join(env_args),
            "BUCKET": bucket_info["name"],
            "MAX_AGE": bucket_info["max_age"],
        }
        workflow.add_job(arch_job)

    return workflow


@jc.command
@click.option(
    "--singularity-version", "-S", default="4.0.2", help="Singularity version"
)
@click.option("--s3cmd-version", "-s", default="2.3.0", help="s3cmd version")
@click.option(
    "--output-file",
    "-o",
    default="generated_pipeline.yaml",
    help="Which file to write the output to",
)
@click.option(
    "--debug/--no-debug", default=False, help="Show debug logging on the console"
)
def create_jobs(singularity_version, s3cmd_version, output_file, debug):
    debug_output(debug)
    architectures = get_architectures()
    workflow = generate_base_container_workflow(
        singularity_version, s3cmd_version, architectures=architectures
    )
    packages_workflow, cache_population_job_names = generate_packages_workflow(
        architectures
    )
    workflow += packages_workflow
    workflow += generate_clean_cache_workflow(architectures, cache_population_job_names)
    workflow += generate_containers_workflow(workflow, architectures, s3cmd_version)

    for job in [
        j
        for j in workflow.jobs
        if "generate build cache population" in j.name
        or "generate spacktainer" in j.name
    ]:
        logger.debug(f"Adding needs for {job.name}")
        [
            job.add_need(need.name)
            for need in get_arch_or_multiarch_job(workflow, job.architecture)
        ]

    # TODO
    # * rules?
    write_yaml(workflow.to_dict(), output_file)


if __name__ == "__main__":
    jc()
