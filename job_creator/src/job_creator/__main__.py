import glob
import logging
import logging.config
import os

import click
from natsort import natsorted

from job_creator.architectures import architecture_map
from job_creator.ci_objects import Job, Trigger, Workflow
from job_creator.containers import generate_base_container_workflow
from job_creator.job_templates import clean_cache_yaml
from job_creator.logging_config import LOGGING_CONFIG
from job_creator.packages import generate_packages_workflow
from job_creator.utils import load_yaml, write_yaml

logging.config.dictConfig(LOGGING_CONFIG)
logger = logging.getLogger("job_creator")


@click.group()
def jc():
    pass


@jc.command
@click.option("--pipeline-file", "-f", help="YAML pipeline file generated by spack")
@click.option(
    "--out-dir",
    "-o",
    help="Output dir in which to dump split pipelines. Will be created if necessary.",
)
def process_spack_pipeline(pipeline_file, out_dir):
    """
    Given a spack-generated pipeline file, this will:
      * split it along the generated stages: each stage will become its own workflow
      * in each "stage", do the necessary spack mirror manipulation, variable setting, ...
      * add a job before all the stages run that will collect artifacts needed, so that
        the stages can grab them from within the same workflow
      * configure "stage" dependencies
    """
    print("Processing spack pipeline")
    architecture = out_dir.split(".")[1]
    pipeline = load_yaml(pipeline_file)

    if "no-specs-to-rebuild" in pipeline:
        write_yaml(pipeline, "spack_pipeline.yaml")
        return

    split_pipelines = {
        stage: {"variables": pipeline["variables"]} for stage in pipeline["stages"]
    }
    for name, item in pipeline.items():
        if name == "variables" and "variables" in architecture_map[architecture]:
            item.update(architecture_map[architecture]["variables"])
        if name in ["stages", "variables"]:
            continue
        item.pop("needs", None)
        stage = item.pop("stage", "no stage")
        job = Job(name=name, architecture=architecture, **item)

        job.add_spack_mirror()
        job.set_aws_variables()

        job.image["pull_policy"] = "always"
        job.needs = [
            {
                "pipeline": os.environ.get("CI_PIPELINE_ID"),
                "job": f"generate build cache population job for {architecture}",
                "artifacts": True,
            }
        ]
        split_pipelines[stage][name] = job.to_dict()

    build_workflow = Workflow()

    if not os.path.exists(out_dir):
        os.makedirs(out_dir, exist_ok=True)

    collect_job = "collect artifacts"
    job = Job(
        collect_job,
        architecture,
        needs=[
            {
                "pipeline": os.environ.get("CI_PIPELINE_ID"),
                "job": f"process spack pipeline for {architecture}",
                "artifacts": True,
            }
        ],
        script=[
            "cat spack_pipeline.yaml",
            f"find artifacts.{architecture}",
        ],
        stage="run spack-generated pipelines",
        artifacts={"when": "always", "paths": ["*.yaml", "artifacts.*"]},
    )
    build_workflow.add_job(job)

    previous_stage = None
    for stage, stage_pipeline in natsorted(split_pipelines.items()):
        logger.debug(f"Adding stage {stage}")
        if len(stage_pipeline) > 1:
            pipeline_file = f"{out_dir}/pipeline-{stage}.yaml"
            needs = [
                {
                    "job": f"{collect_job} for {architecture}",
                    "artifacts": True,
                },
            ]

            if previous_stage:
                needs.append({"job": previous_stage})

            previous_stage = stage

            trigger = Trigger(
                name=stage,
                trigger={
                    "include": [
                        {
                            "artifact": pipeline_file,
                            "job": f"{collect_job} for {architecture}",
                        }
                    ],
                    "strategy": "depend",
                },
                needs=needs,
                stage=stage,
            )
            build_workflow.add_trigger(trigger)
            write_yaml(stage_pipeline, pipeline_file)

    write_yaml(build_workflow.to_dict(), "spack_pipeline.yaml")


def generate_clean_cache_workflow(architectures):
    """
    Generate the jobs to clean the build cache
    """
    workflow = Workflow()
    stage = "clean build cache"
    workflow.stages = [stage]
    for architecture in architectures:
        arch_job = Job(
            "clean build cache",
            architecture=architecture,
            stage=stage,
            **clean_cache_yaml,
        )

        bucket_info = architecture_map[architecture]["cache_bucket"]
        arch_job.needs = [
            {
                "job": f"generate build cache population job for {architecture}",
                "artifacts": True,
            }
        ]
        arch_job.variables = {
            "SPACK_ENV": f"jobs_scratch_dir.{architecture}/concrete_environment/spack.lock",
            "BUCKET": bucket_info["name"],
            "MAX_AGE": bucket_info["max_age"],
        }
        workflow.add_job(arch_job)

    return workflow


@jc.command
@click.option(
    "--singularity-version", "-S", default="4.0.2", help="Singularity version"
)
@click.option("--s3cmd-version", "-s", default="2.3.0", help="s3cmd version")
@click.option(
    "--output-file",
    "-o",
    default="generated_pipeline.yaml",
    help="Which file to write the output to",
)
def create_jobs(singularity_version, s3cmd_version, output_file):
    architectures = [
        os.path.basename(archdir) for archdir in glob.glob("container_definitions/*")
    ]

    workflow = generate_base_container_workflow(
        singularity_version, s3cmd_version, architectures=architectures
    )
    workflow += generate_packages_workflow(architectures)
    workflow += generate_clean_cache_workflow(architectures)

    logger.debug("Merging packages workflow")
    for job in [
        j for j in workflow.jobs if "generate build cache population" in j.name
    ]:
        multiarch_job_name = "create multiarch for builder"
        builder_job_names = {
            architecture: f"build builder for {architecture}"
            for architecture in architectures
        }
        if multiarch_job_name in workflow:
            job.needs.append(multiarch_job_name)
        elif builder_job_names[job.architecture] in workflow:
            job.needs.append(builder_job_names[job.architecture])

    # TODO
    # * rules?
    write_yaml(workflow.to_dict(), output_file)


if __name__ == "__main__":
    jc()
